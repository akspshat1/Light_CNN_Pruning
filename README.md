****Lightweight CNN Pruning and Knowledge Distillation****
This project focuses on enhancing the performance and efficiency of Convolutional Neural Networks (CNNs) through pruning and knowledge distillation techniques. The pipeline includes data augmentation, pre-training, and detailed performance evaluation. Custom CNNs, as well as ResNet18, were fine-tuned and configured for efficient image classification tasks.

**Key Features**
Pruning Techniques: Applied pruning to reduce the model size and computational cost while maintaining high accuracy.
Knowledge Distillation: Utilized knowledge distillation to transfer knowledge from a larger, well-trained model to a smaller, more efficient one.
Data Augmentation: Incorporated data augmentation strategies to improve model generalization and robustness.
Pre-Training and Fine-Tuning: Configured and fine-tuned ResNet18 and custom lightweight CNN architectures for optimal performance.
Performance Evaluation: Comprehensive evaluation of models using various metrics to ensure the balance between efficiency and accuracy.

# Light-Weight-CNN
References : 
https://arxiv.org/abs/1707.06168
